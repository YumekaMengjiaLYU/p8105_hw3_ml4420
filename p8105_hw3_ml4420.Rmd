---
title: "p8105_hw3_ml4420"
author: Mengjia Lyu
date: 2019-10-13
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)


knitr::opts_chunk$set(
        echo = TRUE,
        warning = FALSE,
        fig.width = 8,
        fig.height = 6,
        out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))

```

# Problem 1



```{r exploratory data analysis}

# load the data
data("instacart")

# check the number of rows and columns
dim(instacart)

# view the structure of data
str(instacart)

# describe key variables
names(instacart)

# illustrative examples of observations
hist(instacart$order_hour_of_day)
```
The dataset contains 1384617 rows and 15 columns. Key variables include 

+ order_id: order identifier
+ product_id: product identifier
+ user_id: customer identifier
+ order_dow: the day of the week on which the order was placed
+ order_hour_of_day: the hour of the day on which the order was placed
+ product_name: name of the product
+ aisle_id: aisle identifier
+ department_id: department identifier

From the illustration of the *order_hour_of_day* variable, we can see that 13-14PM is the most popular time. 


```{r}
instacart %>%
  summarise(n_distinct(aisle)) %>% # return the number of aisles
  
tail(names(sort(table(instacart$aisle))), 1) # return the aisle which most items ordered from

tab1 = instacart %>% 
  group_by(aisle) %>%
  mutate(n_aisle = n()) %>%
  filter(n_aisle > 10000) %>%
  ggplot(aes(x = aisle)) + geom_bar(color = "lightblue", fill = "pink") + theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 13), axis.title.y = element_text(size = 13), axis.title = element_text(size = 14, face = "bold")) +
      labs(title = "Number of Items Ordered in Each Aisle", caption = "Limited to aisles with more than 10000 items ordered")
  

# create a table for three most popular items in baking/dog food/packaged vegetable fruit
tab_most_ppl = instacart  %>%
  filter(aisle == "dog food care" | aisle == "packaged vegetables fruits" | aisle == "baking ingredients") %>%
  group_by(aisle) %>%
  count(product_name) %>%
  arrange(desc(n))%>%
  top_n(3) 
knitr::kable(tab_most_ppl)

# create a table for the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream ordered on each day of the week
tab_mean_hour = instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  select(product_name, order_dow, order_hour_of_day) %>% # select the correct columns
  group_by(order_dow, product_name) %>%
  mutate(mean_hour = mean(order_hour_of_day)) %>%
  select(-order_hour_of_day) %>%
  distinct() %>%
  pivot_wider( # pivot wider 
    names_from = order_dow,
    values_from = mean_hour
  ) 

knitr::kable(tab_mean_hour)
 
```

# Problem 2

```{r}
# load data from Behavioral Risk Factors Surveillance Systems
data("BRFSS")

# tidy data
BRFSS_tidy_data = brfss_smart2010 %>%
 janitor::clean_names() %>% # format the data to use appropriate variable names
 filter(topic == "Overall Health") %>% # focus on the "Overall Health" topic
 filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% # include only responses from "Excellent" to "Poor"
 mutate(response = factor(response, ordered = TRUE, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) # ordered levels

# search for states observed at 7 or more locations in 2002
states_2002 = BRFSS_tidy_data %>%
 filter(year == 2002) %>%
 select(locationabbr, locationdesc) %>%
 arrange(locationabbr, locationdesc) %>%
 distinct() %>%
 group_by(locationabbr) %>%
 count() %>%
 filter(n > 7 | n == 7)

 # list out the states  
list(pull(states_2002, locationabbr))

```
We can see that CT, FL, MA, NC, NJ, PA were observed at 7 or more locations in 2002.

```{r}
# search for states observed at 7 or more locations in 2010
states_2010 = BRFSS_tidy_data %>%
 filter(year == 2010) %>%
 select(locationabbr, locationdesc) %>%
 arrange(locationabbr, locationdesc) %>%
 distinct() %>%
 group_by(locationabbr) %>%
 count() %>%
 filter(n > 7 | n == 7)

# list out states
list(pull(states_2010, locationabbr)) 
```
We can see that CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, WA were observed at 7 or more locations in 2010.

```{r}
# create a sphagetti plot
sphagetti_plot = BRFSS_tidy_data %>%
 filter(response == "Excellent") %>% # limited to "Excellent" responses
 select(year, locationabbr, locationdesc, data_value) %>%
 arrange(locationabbr, locationdesc) %>%
 group_by(locationabbr, year) %>%
 summarise(avg_data_value = mean(data_value)) %>%
 ggplot(aes(x = year, y = avg_data_value, color = factor(locationabbr))) + geom_line(aes(group = locationabbr)) +
 ggtitle(label = "Average Overall Health Value Over Time Within Each State") +
 theme_minimal() + theme(plot.title = element_text(hjust = 0.5, lineheight = .8, face = "bold")) +
 xlab("Year") + ylab("Average Value")

sphagetti_plot

# create a two panel bar plot
two_panel_plot = BRFSS_tidy_data %>%
 filter(locationabbr == "NY" & year %in% c(2006, 2010)) %>%
 select(data_value, response, year, locationdesc) %>%
 ggplot(aes(x = response, y = data_value, fill = locationdesc)) + # use fill to differentiate counties
 geom_bar(stat = "identity", position = "dodge") + facet_grid(year ~ .) + # use dodge to place two graphs side by side
 ggtitle(label = "Distribution of Overall Health Value for Responses from 'Poor' to 'Excellent'") +
 theme_minimal() + theme(plot.title = element_text(hjust = 0.5, lineheight = .8, face = "bold")) +
 xlab("Responses") + ylab("Overall Health Value")

two_panel_plot 
```

# Problem 3
```{r}
# load and clean data
accel_data = read_csv(file = "./data/accel_data.csv") %>%
 janitor::clean_names() %>%
 mutate(day_type = plyr::mapvalues(day, from = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"),
 to = c("Weekday", "Weekday", "Weekday", "Weekday", "Weekday", "Weekend", "Weekend"))) %>%
 mutate(day = factor(day)) # encode day into factor variable since a week always has 7 unique day values
```